{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unauthorized-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from PIL import ImageEnhance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-spectacular",
   "metadata": {},
   "source": [
    "def transpose_image(path):\n",
    "    image_dir_path = path\n",
    "    images = glob.glob(image_dir_path + \"/*.jpg\")\n",
    "    count = 0\n",
    "    for img in images:\n",
    "        sn = random.choice([0, 1, 2])\n",
    "        if sn == 0:\n",
    "            old_img = Image.open(img)\n",
    "            new_img = old_img.transpose(Image.ROTATE_90)\n",
    "            new_img.save(\"{}/r90_{}.jpg\".format(image_dir_path, count), \"JPEG\")\n",
    "        if sn == 1:\n",
    "            old_img = Image.open(img)\n",
    "            new_img = old_img.transpose(Image.ROTATE_180)\n",
    "            new_img.save(\"{}/r180_{}.jpg\".format(image_dir_path, count), \"JPEG\")\n",
    "        if sn == 2:\n",
    "            old_img = Image.open(img)\n",
    "            new_img = old_img.transpose(Image.ROTATE_270)\n",
    "            new_img.save(\"{}/r270_{}.jpg\".format(image_dir_path, count), \"JPEG\")\n",
    "        count+=1\n",
    "\n",
    "\n",
    "def sharpness_image(path):\n",
    "    image_dir_path = path\n",
    "    images = glob.glob(image_dir_path + \"/*.jpg\")\n",
    "    count = 0\n",
    "    for img in images:\n",
    "        old_img = Image.open(img)\n",
    "        enhancer = ImageEnhance.Sharpness(old_img)\n",
    "        new_img = enhancer.enhance(random.choice([0.5, 0.75, 1.5, 2]))\n",
    "        new_img.save(\"{}/s_{}.jpg\".format(image_dir_path, count), \"JPEG\")\n",
    "        count+=1\n",
    "\n",
    "\n",
    "def brightness_image(path):\n",
    "    image_dir_path = path\n",
    "    images = glob.glob(image_dir_path + \"/*.jpg\")\n",
    "    count = 0\n",
    "    for img in images:\n",
    "        old_img = Image.open(img)\n",
    "        enhancer = ImageEnhance.Brightness(old_img)\n",
    "        new_img = enhancer.enhance(random.choice([0.5, 0.75, 1.5, 2]))\n",
    "        new_img.save(\"{}/b_{}.jpg\".format(image_dir_path, count), \"JPEG\")\n",
    "        count+=1\n",
    "\n",
    "\n",
    "def contrast_image(path):\n",
    "    image_dir_path = path\n",
    "    images = glob.glob(image_dir_path + \"/*.jpg\")\n",
    "    count = 0\n",
    "    for img in images:\n",
    "        old_img = Image.open(img)\n",
    "        enhancer = ImageEnhance.Contrast(old_img)\n",
    "        new_img = enhancer.enhance(random.choice([0.5, 0.75, 1.5, 2]))\n",
    "        new_img.save(\"{}/c_{}.jpg\".format(image_dir_path, count), \"JPEG\")\n",
    "        count+=1       \n",
    "\n",
    "\n",
    "def resize_image(path):\n",
    "    #image_dir_path = os.getenv(\"HOME\") + path\n",
    "    image_dir_path = path\n",
    "    images = glob.glob(image_dir_path + \"/*.jpg\")  \n",
    "    target_size=(28, 28)\n",
    "    for img in images:\n",
    "        old_img = Image.open(img)\n",
    "        new_img = old_img.resize(target_size, Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-alexander",
   "metadata": {},
   "source": [
    "이미지에 변화를 주어 데이터셋에 추가하는 과정을 진행해보기 위해 만들었던 함수\n",
    "정확도 향상에 크게 기여하진 않지만 조금의 향상된 결과를 볼 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-leisure",
   "metadata": {},
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp3/rock\"\n",
    "resize_image(image_dir_path)\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp3/scissor\"\n",
    "resize_image(image_dir_path)\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp3/paper\"\n",
    "resize_image(image_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-field",
   "metadata": {},
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp2/rock\"\n",
    "transpose_image(image_dir_path)\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp2/scissor\"\n",
    "transpose_image(image_dir_path)\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp2/paper\"\n",
    "transpose_image(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp2/rock\"\n",
    "contrast_image(image_dir_path)\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp2/scissor\"\n",
    "contrast_image(image_dir_path)\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp2/paper\"\n",
    "contrast_image(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp2/rock\"\n",
    "brightness_image(image_dir_path)\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp2/scissor\"\n",
    "brightness_image(image_dir_path)\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp2/paper\"\n",
    "brightness_image(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp2/rock\"\n",
    "sharpness_image(image_dir_path)\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp2/scissor\"\n",
    "sharpness_image(image_dir_path)\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp2/paper\"\n",
    "sharpness_image(image_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "golden-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(img_path, number_of_data):\n",
    "    img_size = 28\n",
    "    color = 3\n",
    "    \n",
    "    imgs = np.zeros(number_of_data * img_size * img_size * color, dtype = np.int32).reshape(number_of_data, img_size, img_size, color)\n",
    "    labels = np.zeros(number_of_data, dtype = np.int32)\n",
    "\n",
    "    idx = 0\n",
    "    for file in glob.iglob(img_path + '/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file), dtype = np.int32)\n",
    "        imgs[idx, :, :, :] = img\n",
    "        labels[idx] = 0\n",
    "        idx = idx + 1\n",
    "\n",
    "    for file in glob.iglob(img_path + '/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file), dtype = np.int32)\n",
    "        imgs[idx, :, :, :] = img\n",
    "        labels[idx] = 1\n",
    "        idx = idx + 1       \n",
    "    \n",
    "    for file in glob.iglob(img_path + '/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file), dtype = np.int32)\n",
    "        imgs[idx, :, :, :] = img\n",
    "        labels[idx] = 2\n",
    "        idx = idx + 1\n",
    "        \n",
    "    return imgs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "working-certification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 128)       3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 905,475\n",
      "Trainable params: 905,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_conv1=128\n",
    "n_conv2=256\n",
    "n_conv3=256\n",
    "n_dense=64\n",
    "n_epoch=15\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_conv1, (3, 3), activation = 'relu', input_shape=(28, 28, 3)))\n",
    "model.add(keras.layers.MaxPooling2D(2, 2))\n",
    "model.add(keras.layers.Conv2D(n_conv2, (3, 3), activation = 'relu'))\n",
    "model.add(keras.layers.MaxPooling2D(2, 2))\n",
    "model.add(keras.layers.Conv2D(n_conv3, (3, 3), activation = 'relu'))\n",
    "model.add(keras.layers.MaxPooling2D(2, 2))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation = 'relu'))\n",
    "model.add(keras.layers.Dense(3, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-criminal",
   "metadata": {},
   "source": [
    "Param # 가중치와 편향 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-harassment",
   "metadata": {},
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp\"\n",
    "(x_train, y_train)=load_data(image_dir_path, 3900)\n",
    "x_train_norm = x_train/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "congressional-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp\"\n",
    "(X, y)=load_data(image_dir_path, 3900)\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=1)\n",
    "x_train_norm = x_train/255.0 #normalization\n",
    "x_test_norm = x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "indirect-perth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "98/98 [==============================] - 10s 51ms/step - loss: 1.1031 - accuracy: 0.3493\n",
      "Epoch 2/15\n",
      "98/98 [==============================] - 1s 9ms/step - loss: 1.0325 - accuracy: 0.4456\n",
      "Epoch 3/15\n",
      "98/98 [==============================] - 1s 9ms/step - loss: 0.6530 - accuracy: 0.7150\n",
      "Epoch 4/15\n",
      "98/98 [==============================] - 1s 10ms/step - loss: 0.4288 - accuracy: 0.8406\n",
      "Epoch 5/15\n",
      "98/98 [==============================] - 1s 10ms/step - loss: 0.2939 - accuracy: 0.8896\n",
      "Epoch 6/15\n",
      "98/98 [==============================] - 1s 10ms/step - loss: 0.1953 - accuracy: 0.9244\n",
      "Epoch 7/15\n",
      "98/98 [==============================] - 1s 10ms/step - loss: 0.1016 - accuracy: 0.9658\n",
      "Epoch 8/15\n",
      "98/98 [==============================] - 1s 10ms/step - loss: 0.0539 - accuracy: 0.9840\n",
      "Epoch 9/15\n",
      "98/98 [==============================] - 1s 10ms/step - loss: 0.0656 - accuracy: 0.9803\n",
      "Epoch 10/15\n",
      "98/98 [==============================] - 1s 10ms/step - loss: 0.0647 - accuracy: 0.9765\n",
      "Epoch 11/15\n",
      "98/98 [==============================] - 1s 10ms/step - loss: 0.0707 - accuracy: 0.9801\n",
      "Epoch 12/15\n",
      "98/98 [==============================] - 1s 10ms/step - loss: 0.0292 - accuracy: 0.9921\n",
      "Epoch 13/15\n",
      "98/98 [==============================] - 1s 10ms/step - loss: 0.0313 - accuracy: 0.9911\n",
      "Epoch 14/15\n",
      "98/98 [==============================] - 1s 10ms/step - loss: 0.0288 - accuracy: 0.9940\n",
      "Epoch 15/15\n",
      "98/98 [==============================] - 1s 10ms/step - loss: 0.0104 - accuracy: 0.9976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe2e46ecd50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(x_train_norm, y_train, epochs = n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "monthly-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp\" #train_test_split 함수를 이용한 테스트데이터\n",
    "(X, y)=load_data(image_dir_path, 3900)\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=1)\n",
    "x_train_norm = x_train/255.0\n",
    "x_test_norm = x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "herbal-material",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 2s - loss: 0.0167 - accuracy: 0.9936\n",
      "loss : 0.016730744391679764\n",
      "accuracy : 0.9935897588729858\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_norm, y_test, verbose = 2)\n",
    "print(\"loss : {}\".format(test_loss))\n",
    "print(\"accuracy : {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "apart-private",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp/test\" #학습 전 데이터셋에서 직접 골라낸 이미지 테스트데이터 300개\n",
    "(x_test, y_test)=load_data(image_dir_path, 300)\n",
    "x_test_norm = x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "earlier-draft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 0.0232 - accuracy: 0.9900\n",
      "loss : 0.023226678371429443\n",
      "accuracy : 0.9900000095367432\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_norm, y_test, verbose = 2)\n",
    "print(\"loss : {}\".format(test_loss))\n",
    "print(\"accuracy : {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "modular-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp3\" #새로운 데이터셋\n",
    "(x_test, y_test)=load_data(image_dir_path, 800)\n",
    "x_test_norm = x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bizarre-supply",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 0s - loss: 2.6241 - accuracy: 0.7513\n",
      "loss : 2.624124050140381\n",
      "accuracy : 0.7512500286102295\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_norm, y_test, verbose = 2)\n",
    "print(\"loss : {}\".format(test_loss))\n",
    "print(\"accuracy : {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tight-yemen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 - 0s - loss: 2.5999 - accuracy: 0.7417\n",
      "loss : 2.599865436553955\n",
      "accuracy : 0.7416666746139526\n"
     ]
    }
   ],
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp3\" #새로운 데이터셋에서 train_test_split으로 240개 테스트데이터 선택\n",
    "(X, y)=load_data(image_dir_path, 800)\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y, test_size=0.3)\n",
    "x_train_norm = x_train/255.0\n",
    "x_test_norm = x_test/255.0\n",
    "test_loss, test_accuracy = model.evaluate(x_test_norm, y_test, verbose = 2)\n",
    "print(\"loss : {}\".format(test_loss))\n",
    "print(\"accuracy : {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-tumor",
   "metadata": {},
   "source": [
    "conv1 128\n",
    "conv2 256\n",
    "conv3 256\n",
    "dense 64\n",
    "epoch 15\n",
    "loss: 0.0281 - accuracy: 0.9962 loss: 0.0143 - accuracy: 0.9933 loss: 1.6875 - accuracy: 0.7887 loss: 1.3083 - accuracy: 0.8333\n",
    "loss: 0.0269 - accuracy: 0.9949 loss: 0.0377 - accuracy: 0.9900 loss: 2.3357 - accuracy: 0.7325 loss: 2.0729 - accuracy: 0.7458\n",
    "loss: 0.0496 - accuracy: 0.9859 loss: 0.0485 - accuracy: 0.9800 loss: 2.2167 - accuracy: 0.7875 loss: 1.9866 - accuracy: 0.8083\n",
    "\n",
    "conv1 128\n",
    "conv2 256\n",
    "conv3 256\n",
    "dense 64\n",
    "epoch 30\n",
    "loss: 0.0262 - accuracy: 0.9962 loss: 0.0069 - accuracy: 0.9967 loss: 2.2958 - accuracy: 0.7800 loss: 2.2688 - accuracy: 0.7792\n",
    "loss: 0.0144 - accuracy: 0.9962 loss: 0.0272 - accuracy: 0.9933 loss: 3.5359 - accuracy: 0.7588 loss: 3.4498 - accuracy: 0.7708\n",
    "loss: 0.0031 - accuracy: 0.9987 loss: 0.0352 - accuracy: 0.9933 loss: 2.3547 - accuracy: 0.8025 loss: 2.6465 - accuracy: 0.8083\n",
    "\n",
    "conv1 32\n",
    "conv2 64\n",
    "conv3 128\n",
    "dense 64\n",
    "epoch 15\n",
    "loss: 0.0542 - accuracy: 0.9897 loss: 0.0340 - accuracy: 0.9833 loss: 1.8093 - accuracy: 0.7450 loss: 2.1536 - accuracy: 0.7000\n",
    "loss: 0.0861 - accuracy: 0.9731 loss: 0.1016 - accuracy: 0.9567 loss: 1.0858 - accuracy: 0.7950 loss: 0.8624 - accuracy: 0.8125\n",
    "loss: 0.0672 - accuracy: 0.9821 loss: 0.0340 - accuracy: 0.9967 loss: 2.0408 - accuracy: 0.6562 loss: 1.7934 - accuracy: 0.6792\n",
    "\n",
    "conv1 512\n",
    "conv2 1024\n",
    "conv3 1024\n",
    "dense 512\n",
    "epoch 15\n",
    "loss: 0.0886 - accuracy: 0.9769 loss: 0.0543 - accuracy: 0.9833 loss: 2.6290 - accuracy: 0.6875 loss: 2.5726 - accuracy: 0.6708\n",
    "loss: 0.0382 - accuracy: 0.9936 loss: 0.0369 - accuracy: 0.9867 loss: 2.6970 - accuracy: 0.7475 loss: 2.8757 - accuracy: 0.7208\n",
    "loss: 0.0552 - accuracy: 0.9846 loss: 0.0331 - accuracy: 0.9867 loss: 3.4652 - accuracy: 0.6375 loss: 3.5575 - accuracy: 0.6583\n",
    "\n",
    "\n",
    "normalization x\n",
    "conv1 128\n",
    "conv2 256\n",
    "conv3 256\n",
    "dense 64\n",
    "epoch 15\n",
    "loss: 0.1763 - accuracy: 0.9372 loss: 0.2103 - accuracy: 0.9100 loss: 3.1583 - accuracy: 0.5863 loss: 3.5713 - accuracy: 0.5750\n",
    "epoch 30\n",
    "loss: 0.1041 - accuracy: 0.9910 loss: 0.0264 - accuracy: 0.9900 loss: 2.1943 - accuracy: 0.7312 loss: 2.3874 - accuracy: 0.7250\n",
    "epoch 40\n",
    "loss: 0.0084 - accuracy: 0.9949 loss: 0.0402 - accuracy: 0.9933 loss: 3.3139 - accuracy: 0.7550 loss: 2.5302 - accuracy: 0.7875\n",
    "\n",
    "\n",
    "conv1 128\n",
    "conv2 256\n",
    "dense 64\n",
    "epoch 15\n",
    "loss: 0.0696 - accuracy: 0.9859 loss: 0.0278 - accuracy: 0.9933 loss: 2.0714 - accuracy: 0.6725 loss: 2.0650 - accuracy: 0.6708\n",
    "epoch 30\n",
    "loss: 0.0522 - accuracy: 0.9974 loss: 0.0163 - accuracy: 0.9967 loss: 3.0442 - accuracy: 0.6762 loss: 2.7411 - accuracy: 0.7083\n",
    "epoch 50\n",
    "loss: 0.0631 - accuracy: 0.9962 loss: 0.0037 - accuracy: 1.0000 loss: 4.2513 - accuracy: 0.7025 loss: 5.3597 - accuracy: 0.6667\n",
    "\n",
    "conv1 128\n",
    "dense 64\n",
    "epoch 15\n",
    "loss: 0.0757 - accuracy: 0.9872 loss: 0.0358 - accuracy: 0.9933 loss: 2.6967 - accuracy: 0.5788 loss: 2.8090 - accuracy: 0.6000\n",
    "epoch 30\n",
    "loss: 0.0479 - accuracy: 0.9962 loss: 0.0172 - accuracy: 0.9967 loss: 3.8713 - accuracy: 0.5612 loss: 3.9520 - accuracy: 0.5500\n",
    "epoch 5\n",
    "loss: 0.2757 - accuracy: 0.9090 loss: 0.2668 - accuracy: 0.8933 loss: 1.4980 - accuracy: 0.4988 loss: 1.4372 - accuracy: 0.5167"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-tunnel",
   "metadata": {},
   "source": [
    "하이퍼파라미터 값에 따라 결과가 조금씩 달라지긴 하지만 큰 변화를 보이지는 않았다.\n",
    "적당한 하이퍼파라미터로 이미지에서 특징추출을 해주어야 한다. 너무 작거나 크면 특징추출이 잘 안되고 하이퍼파라미터가 클수록 연산량이 많아서 학습시간이 길어지기 때문에 100 ~ 200 근처의 값이 적당한 값이라 한다.\n",
    "\n",
    "정규화를 통해 입력 데이터를 유사한 분포를 가지도록 하는 것이 좋다. 정규화는 학습과정에서 수렴을 더 쉽고 빠르게 만들어준다.\n",
    "정규화를 했을 때 epoch 15로 충분했지만 정규화를 하지 않았을 때는 epoch 30 ~ 40 정도로 높여야 비슷한 결과를 볼 수 있었다.\n",
    "\n",
    "레이어 개수를 1 ~ 3개 바꾸어 보았을 때 하나의 데이터셋을 이용 할 때는 큰 차이가 없지만 새로운 데이터셋을 이용하여 테스트 해 보았을 때 레이어 개수가 많을수록 정확도가 높게 나타났다. 레이어 개수가 많을수록 성능이 높아지지만 너무 많으면 의사결정을 하지 못하는 경우가 생길 수 있다고 한다.\n",
    "\n",
    "동일한 데이터셋, 파라미터 사용하여 반복 수행해도 정확도가 조금씩 달라진다.\n",
    "model.fit() 함수에 기본값으로 shuffle=True 설정이 되어있어서 데이터가 순서나 특정 규칙에 따라 분포하는 경우 이러한 특성이 학습될 수 있기 때문에 shuffle을 통해 분포가 고르게 만들어 학습 성능을 높인다.\n",
    "\n",
    "모델을 학습하고 시험하는데 있어 가장 중요한 것은 데이터셋이다. 데이터셋이 다양하게 많을수록 좋은 모델로 학습시키고 시험성능이 향상된다.\n",
    "train_test_split에서 테스트사이즈를 증가시키면 시험성능이 낮아졌다. 학습량도 줄고 학습하지 못한 테스트데이터도 늘어나다보니 그런것같다. 학습데이터와 다른 데이터셋을 이용해 시험할 경우 시험데이터에 따라 정확도가 크게 차이났다. 좀 더 크고 화질이 좋은 이미지로 학습데이터와 모델을 개선한다면 새로운 시험 데이터에 대한 정확도도 좋아지겠지만, 이번 과제에서는 28x28 크기의 작은 이미지를 이용하여 구분하기 힘든 데이터도 있었고, 데이터 분포가 크지않은 데이터셋을 이용하였다. 그러다 보니 배경이나 밝기 등 주변 환경이 생소한 시험데이터의 경우 정확도가 많이 떨어졌다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
